{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's dive deep into Feed-Forward Networks!**\n",
    "ðŸ¦«KURW.AIðŸ’—\n",
    "\n",
    "---\n",
    "\n",
    "Venchislaw 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Intro for Nerds\n",
    "---\n",
    "\n",
    "Human brain is incredible.<br>\n",
    "It can learn complex patterns by building complex neuron connections.<br>\n",
    "Yet we don't 100% now how brain works, we can build digital model of neuron and __network of neurons__<br>\n",
    "To build digital model of Neuron we should understand how biological one works.<br>\n",
    "I'm not a neuro-scientist, but I remember biology lesson at school about neurons<br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*K1ee1SzB0lxjIIo7CGI7LQ.png\" width=30%><br>\n",
    "Ommiting biological details we can describe neuron in few words as a cell recieving information with dendrites from past neurons and either \"firing\" electrical signal or not. Neuron then passes its processed information with synapses to the following neurons.<br>\n",
    "Once again, model requires simplification, so we can look at a neuron as at a \"tiny unit that intakes some information and returns processed inputs.<br>\n",
    "The only question is \"processing\".<br>\n",
    "What should we do?<br>\n",
    "Neural Network consists of neurons, so it's better to think about one single unit.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Neuron Model\n",
    "\n",
    "---\n",
    "\n",
    "To the moment of building neuran model humanity was already aware of \"linear models\".<br>\n",
    "That's exactly what scientists decided to use.<br>\n",
    "For the model of neuron they picked linear model with \"activation function\" applied to it.<br>\n",
    "If you don't remember how linear model looks I can remind you quickly:<br>\n",
    "$$a = \\sum^n_{i=0} \\theta _ix_i$$\n",
    "where $x_0 = 1$ (to learn bias)<br>\n",
    "What is this \"activation function\"?<br>\n",
    "Biological Neuron is binary, it's either on or off.<br>\n",
    "Activation Function in this case is a __step function__.<br>\n",
    "It's 1 for input > threshold and 0 for input < threshold.<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/HardLimitFunction.png/400px-HardLimitFunction.png\" width=30%><br>\n",
    "As you can see from this picture we have a 0 threshold.<br>\n",
    "However, custom threshold is not a problem for us.<br>\n",
    "**Threshold = - bias**\n",
    "This is to say, our linear computation already includes threshold.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Perceptron\n",
    "\n",
    "Perceptron is the first model of biological neuron.<br>\n",
    "Invented in 1958 it applied all practices described above with a pretty interesting learning rule.<br>\n",
    "Perceptron model is used for supervised binary classification task.<br>\n",
    "__How it works:__<br>\n",
    "It does absolutely the same thing, but to tune hyperparameters it follows the following rule:<br>\n",
    "- If prediction is correct (1-1 or 0-0) we do not change weights\n",
    "- If prediction is wrong:\n",
    "- - False Positive:<br>\n",
    "        This means our linear output is over the threshold<br>\n",
    "        We subtract scaled input feature vector from parameters<br>\n",
    "- - False Negative:<br>\n",
    "        This means our linear output is under the threshold<br>\n",
    "        We add up scaled input feature vector to parameters<br>\n",
    "\n",
    "**NOTE: Perceptron process each sample individually.**<br>\n",
    "This method tunes parameters close enough to the desired values.<br>\n",
    "### Let's build one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(1000, 10) (1000,)\n",
      "==================================================\n",
      "[[ 0.66698806  0.02581308 -0.77761941  0.94863382  0.70167179 -1.05108156\n",
      "  -0.36754812 -1.13745969 -1.32214752  1.77225828]\n",
      " [-0.34745899  0.67014016  0.32227152  0.06034293 -1.04345    -1.00994188\n",
      "   0.44173637  1.12887685 -1.83806777 -0.93876863]\n",
      " [-0.20184052  1.04537128  0.53816197  0.81211867  0.2411063  -0.95250953\n",
      "  -0.13626676  1.26724821  0.17363364 -1.22325477]\n",
      " [ 1.41531998  0.45771098  0.72887584  1.96843473 -0.54778801 -0.67941827\n",
      "  -2.50623032  0.14696049  0.60619549 -0.02253889]\n",
      " [ 0.01342226  0.93594489  0.42062266  0.41161964 -0.07132392 -0.04543758\n",
      "   1.04088597 -0.09403473 -0.42084395 -0.55198856]] [0 0 1 0 1]\n",
      "==================================================\n",
      "[[  0 478]\n",
      " [  1 522]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here I generate simple dummy dataset.<br>\n",
    "\"\"\"\n",
    "np.random.seed(23)\n",
    "\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "weights = np.random.randn(num_features)\n",
    "bias = 0.5\n",
    "y = (np.dot(X, weights) + bias > 0).astype(int)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(X.shape, y.shape)\n",
    "print(\"=\"*50)\n",
    "print(X[:5], y[:5])\n",
    "print(\"=\"*50)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And Here I build Perceptron Neuron!\n",
    "\"\"\"\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros((1, X.shape[1]))\n",
    "\n",
    "        for sample in zip(X, y):\n",
    "            prediction = np.dot(self.weights, sample[0]) + self.bias\n",
    "            activated = prediction > 0\n",
    "\n",
    "            self.weights += (sample[1] - activated) * sample[0]\n",
    "            self.bias += (sample[1] - activated) * 1\n",
    "        \n",
    "        return self.weights, self.bias\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = np.dot(self.weights, X.T) + self.bias\n",
    "        return np.array(prediction > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-13.58053614,  -0.07006142,  -0.82743697,  10.69942632,\n",
       "           1.26320988,  -8.41174634,   2.87887913,  -6.40411674,\n",
       "           6.93203191,  -4.31050118]]),\n",
       " array([2]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = perceptron.predict(X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "wrong = 0\n",
    "for pair in zip(y, y_pred):\n",
    "    if pair[0] != pair[1]:\n",
    "        wrong += 1\n",
    "    \n",
    "print(wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73 Misclassifications are not bad, but...<br>\n",
    "This is the training set it's seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Weakness of Perceptrons.\n",
    "\n",
    "Perceptrons are not suitable for neural networks.<br>\n",
    "1) Good Weights do not guarantee good outputs.\n",
    "2) Sample-by-sample processing is inefficient for larger scale\n",
    "3) Learning Rule of perceptron can not be scaled for multi-layer architecture (of a neural net)\n",
    "\n",
    "You can hear people calling Feed-Forward Neural Network an **MLP (Multi-Layer Perceptron)**, but it's wrong<br>\n",
    "They have nothing to do with Perceptron unit.<br>\n",
    "This is one of those historical-convention naming issues (like one with Logistic Regression).<br>\n",
    "They use gradient-descent for updates and the chain rule...<br>\n",
    "Oops, spoiler here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Linear Model with fancy activators.\n",
    "\n",
    "**This is a paid content. Subscribe to KurwAI Plus to read premium guides.**<br>\n",
    "F.cking with Ya, Work in progress!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://static-00.iconduck.com/assets.00/beaver-emoji-2048x2019-61w8w8mn.png\" width=10%><br>\n",
    "KurwAI Guides.<br>\n",
    "MIT License.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
